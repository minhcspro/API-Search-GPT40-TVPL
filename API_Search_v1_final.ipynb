{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"Cô giáo đang ăn kem\", \"giáo viên nữ của tôi đang ăn kem\"]\n",
    "\n",
    "model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from googleapiclient.discovery import build\n",
    "from tabulate import tabulate\n",
    "from duckduckgo_search import DDGS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import re\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse, urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để chuyển đổi văn bản thành embedding\n",
    "def get_embedding(text, model):\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_result_google_search(input_text, results, model):\n",
    "    documents = []\n",
    "    for temp in results:\n",
    "        documents.append(temp['title'])\n",
    "    if documents:\n",
    "        # Chuyển đổi các văn bản thành embeddings\n",
    "        embeddings = [get_embedding(doc, model) for doc in documents + [input_text]]\n",
    "        cosine_similarities = cosine_similarity([embeddings[-1]], embeddings[:-1])\n",
    "        # Chuyển độ tương đồng thành phần trăm\n",
    "        similarities_percent = cosine_similarities[0] * 100\n",
    "        # Tìm câu có độ tương đồng cao nhất\n",
    "        max_similarity = max(similarities_percent)\n",
    "        max_index = similarities_percent.tolist().index(max_similarity)\n",
    "        # # In ra kết quả\n",
    "        # for idx, sim in enumerate(similarities_percent):\n",
    "        #     print(f\"Độ tương đồng giữa văn bản đầu vào và văn bản {idx + 1}: {sim:.2f}%\")\n",
    "        # print(f\"\\nCâu có độ tương đồng cao nhất với văn bản đầu vào là câu {max_index + 1} với độ tương đồng {max_similarity:.2f}%: \\\"{documents[max_index]}\\\"\")\n",
    "        return results[max_index]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_result_question(input_text, questions, model):\n",
    "    if not questions:\n",
    "        return input_text\n",
    "    embeddings = [get_embedding(doc, model) for doc in questions + [input_text]]\n",
    "    cosine_similarities = cosine_similarity([embeddings[-1]], embeddings[:-1])\n",
    "\n",
    "    # Chuyển độ tương đồng thành phần trăm\n",
    "    similarities_percent = cosine_similarities[0] * 100\n",
    "\n",
    "    # Tìm câu có độ tương đồng cao nhất\n",
    "    max_similarity = max(similarities_percent)\n",
    "    max_index = similarities_percent.tolist().index(max_similarity)\n",
    "    return questions[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postagging_keyword(input_text):\n",
    "    # Tách từ và gán nhãn từ loại\n",
    "    words = ViTokenizer.tokenize(input_text)\n",
    "    pos_tagged = ViPosTagger.postagging(words)\n",
    "    \n",
    "    # Lấy các từ loại danh từ (N), động từ (V), tính từ (A), danh từ riêng (Ny), số (M) và phân cách (F)\n",
    "    keywords = []\n",
    "    i = 0\n",
    "    while i < len(pos_tagged[0]):\n",
    "        word, pos = pos_tagged[0][i], pos_tagged[1][i]\n",
    "        if pos in ['N', 'V', 'A', 'Ny']:\n",
    "            keywords.append(word.lower())\n",
    "        elif pos == 'M' or (pos == 'F' and word =='/'):\n",
    "            combined_word = word\n",
    "            j = i + 1\n",
    "            while j < len(pos_tagged[0]) and pos_tagged[1][j] in ['M', 'F']:\n",
    "                combined_word += pos_tagged[0][j]\n",
    "                j += 1\n",
    "            keywords.append(combined_word.lower())\n",
    "            i = j - 1\n",
    "        i += 1\n",
    "\n",
    "    # Loại bỏ dấu gạch dưới\n",
    "    keywords = [word.replace('_', ' ') for word in keywords]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(input_text):\n",
    "    # black_list = '-site:https://thuvienphapluat.vn/cong-dong-dan-luat -site:https://thuvienphapluat.vn/van-ban'\n",
    "    # black_list = '-site:https://lawnet.vn -site:https://vksndtc.gov.vn -site:thuenhanuoc.vn -site:https://lawnet.vn/vb -site:https://thuvienphapluat.vn/cong-dong-dan-luat -site:psyme.org -site:youtube.com -site:luatminhkhue.vn -site:https://thuvienphapluat.vn/van-ban -site:https://covanphaply.vn/bang -site:https://vietnamnet.vn'\n",
    "    # + '+site:luatvietnam.vn ' \n",
    "    black_list = ' -site:https://luatvietnam.vn/luat-su-tu-van'\n",
    "    search_query = 'site:luatvietnam.vn ' + input_text + black_list\n",
    "    #  + ' ' + black_list\n",
    "    # search_query = f'site:luatvietnam.vn {input_text} OR site:thuvienphapluat.vn {input_text} {black_list}'\n",
    "    # 'site:luatvietnam.vn' + ' ' + \n",
    "    try:\n",
    "        results = DDGS().text(\n",
    "            keywords = search_query,\n",
    "            region = 'vn-vi',\n",
    "            safesearch= 'on',\n",
    "            timelimit= '7d',\n",
    "            max_results= 5\n",
    "        )\n",
    "        # Biểu thức regex để loại bỏ các URL không mong muốn\n",
    "        regex_pattern = r'-d([1-9]|10)\\.html'\n",
    "        # Lọc kết quả bằng regex\n",
    "        filtered_results = [result for result in results if not re.search(regex_pattern, result['href'])]\n",
    "        return filtered_results\n",
    "    except Exception as e:\n",
    "        print(f\"Rate limit or other exception: {e}\")\n",
    "        # Wait for a random time between 5 to 10 seconds\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "        return google_search(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_data(data):  \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "    # Gửi yêu cầu HTTP GET đến URL\n",
    "    response = requests.get(data.get('href'), headers=headers, verify= False)\n",
    "    # , verify= False\n",
    "    # response = requests.get(data.get('href'), headers=headers, verify=False, allow_redirects=True, max_redirects=10)\n",
    "    soup_data = BeautifulSoup(response.content, 'html5lib', from_encoding= 'utf-8')\n",
    "    return soup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để tìm câu tương đồng nhất\n",
    "def find_most_similar_sentence(postagging, results):\n",
    "    original_words = set(postagging)\n",
    "    # print(original_words)\n",
    "    max_similarity = 0\n",
    "    most_similar_sentence = \"\"\n",
    "    \n",
    "    for sentence in results:\n",
    "        sentence_words = set(postagging_keyword(sentence['title'].lower()))\n",
    "        common_words = original_words.intersection(sentence_words)\n",
    "        similarity = len(common_words)\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_sentence = sentence\n",
    "    \n",
    "    return most_similar_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để tìm câu tương đồng nhất\n",
    "def find_most_similar_question(postagging, results):\n",
    "    original_words = set(postagging)\n",
    "    # print(original_words)\n",
    "    max_similarity = 0\n",
    "    most_similar_sentence = \"\"\n",
    "    \n",
    "    for sentence in results:\n",
    "        sentence_words = set(postagging_keyword(sentence.lower()))\n",
    "        common_words = original_words.intersection(sentence_words)\n",
    "        similarity = len(common_words)\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_sentence = sentence\n",
    "    \n",
    "    return most_similar_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xử lý nhiễu và lấy các phần câu hỏi\n",
    "def find_h2_tags(soup_data):\n",
    "    if soup_data is None:\n",
    "        return []\n",
    "    for toc in soup_data.find_all('div', class_=\"tinvanbanmoi\"):\n",
    "        toc.decompose()\n",
    "\n",
    "    for toc in soup_data.find_all('figcaption'):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('div', class_=\"q_title\"):\n",
    "        toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('p'):\n",
    "        for toc1 in toc.find_all('strong'):\n",
    "            if toc1.text == \"Trả lời:\":\n",
    "                toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('h3', style=\"text-align: center;\"):\n",
    "        toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('p'):\n",
    "        for temp in toc.find_all('span', class_=\"ptie2\"):\n",
    "            toc.decompose()\n",
    "            \n",
    "    for toc in soup_data.find_all('script'):\n",
    "        toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('div', class_=\"gm-gray-gradient p-3 mt-4 mb-4 related_posts\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('p'):\n",
    "        for toc1 in toc.find_all('em'):\n",
    "            if toc1.text.startswith('Xem thêm:'):\n",
    "                toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('em'):\n",
    "        if toc.text.startswith('Xem thêm:'):\n",
    "            toc.decompose()\n",
    "    for toc in soup_data.find_all('aside', class_=\"col-lg-3 col-md-3 col-sm-4 col-xs-12 sidebar\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('div', class_=\"related-article\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('div', class_=\"meta\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('p'):\n",
    "        for temp in toc.find_all('strong'):\n",
    "            if temp.text.startswith('1900 6192'):\n",
    "                toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('div', style=\"text-align:center\"):\n",
    "        for toc1 in soup_data.find_all('em'):\n",
    "            toc1.decompose()\n",
    "                \n",
    "    for toc in soup_data.find_all('div', style=\"background: #f9edd7; padding: 10px; margin-bottom: 10px;\"):\n",
    "            toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('ul', class_=\"ulEvent\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('div', class_=\"box_mucluc\"):\n",
    "        toc.decompose()\n",
    "    \n",
    "    for toc in soup_data.find_all('div', class_=\"tag-pos\"):\n",
    "        toc.decompose()\n",
    "        \n",
    "    for toc in soup_data.find_all('h2', class_=\"lead\"):\n",
    "        toc.decompose() \n",
    "    \n",
    "    for toc in soup_data.find_all('section', class_=\"inner_section\" ):\n",
    "        toc.decompose()   \n",
    "         \n",
    "    for toc in soup_data.find_all('p', class_=\"Normal\", style=\"text-align:right;\"):\n",
    "        toc.decompose()   \n",
    "            \n",
    "    # xử lý download\n",
    "    for table_tag in soup_data.find_all('table', border=\"0\", cellpadding=\"0\", cellspacing=\"0\"):\n",
    "        if 'style' not in table_tag.attrs:\n",
    "            # print(table_tag.text.lower())\n",
    "            table_tag.decompose() \n",
    "        \n",
    "    for toc in soup_data.find_all('p', style=\"text-align: center;\"):\n",
    "        toc.decompose()   \n",
    "                   \n",
    "    for toc in soup_data.find_all('div', style=\"text-align: center;\"):\n",
    "        toc.decompose()  \n",
    "        \n",
    "    for toc in soup_data.find_all('div', style=\"clear:both;text-align:center\"):\n",
    "        toc.decompose()  \n",
    "  \n",
    "    for toc in soup_data.find_all('div', style=\"text-align: right;\"):\n",
    "        toc.decompose() \n",
    "            \n",
    "    for toc in soup_data.find_all('div', class_=\"panel panel-primary\"):\n",
    "        toc.decompose()   \n",
    "    \n",
    "    for toc in soup_data.find_all('p', style=\"text-align: right;\"):\n",
    "        toc.decompose()  \n",
    "    \n",
    "    for toc in soup_data.find_all('p', style=\"text-align: left;\"):\n",
    "        toc.decompose()  \n",
    "    \n",
    "    for toc in soup_data.find_all('p', style=\"text-align:right\"):\n",
    "        toc.decompose() \n",
    "          \n",
    "    h2_tags = []\n",
    "    for tags in soup_data.find_all('div', itemprop=\"articleBody\"):\n",
    "        h2_tags = tags.find_all('h2')\n",
    "        \n",
    "    if h2_tags == []:\n",
    "        for tags in soup_data.find_all('div', class_=\"content_post_single\"):\n",
    "            h2_tags = tags.find_all('h2')\n",
    "            \n",
    "    if h2_tags == []:\n",
    "        h2_tags = soup_data.find_all('h2')\n",
    "        \n",
    "    if h2_tags == []:\n",
    "        for toc in soup_data.find_all('div', class_=\"d-flex justify-content-between time-control\"):\n",
    "            toc.decompose()\n",
    "        \n",
    "        for toc in soup_data.find_all('h6'):\n",
    "            toc.decompose()\n",
    "            \n",
    "        for toc in soup_data.find_all('h5'):\n",
    "            toc.decompose()\n",
    "            \n",
    "        for toc in soup_data.find_all('h4'):\n",
    "            toc.decompose()\n",
    "            \n",
    "        for toc in soup_data.find_all('small'):\n",
    "            toc.decompose()\n",
    "\n",
    "        for toc in soup_data.find_all('a'):\n",
    "            if \"next_post\" in toc.get('class', []):\n",
    "                toc.decompose()\n",
    "\n",
    "        for toc in soup_data.find_all('strong', class_=\"d-block mt-3 mb-3 sapo\"):\n",
    "            toc.decompose()\n",
    "\n",
    "        for toc in soup_data.find_all('div', id=\"cau-hoi\"):\n",
    "            toc.decompose()\n",
    "\n",
    "        for toc in soup_data.find_all('div', style=\"background-color: RGBA(196,231,249,0.3)\"):\n",
    "            toc.decompose()\n",
    "        \n",
    "        h2_tags = soup_data.find_all('h1')\n",
    "    return h2_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cắt từng cụm từ điều đến điều, khoản đến khoản, ..., xử lý các trường hợp khoản 1 đến khoản 5,...\n",
    "def split_by_dieu_new1(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in dieu_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "################\n",
    "def extract_dieu_err(text):\n",
    "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "##############\n",
    "def process_dieu_parts(text):\n",
    "    dieu_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
    "                    dieu_final.append(temp)\n",
    "                else:\n",
    "                    dieu_final.append('Điều ' + temp)\n",
    "\n",
    "    return dieu_final\n",
    "###############\n",
    "def replace_dieu_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_dieu_new1(text)\n",
    "    matches = extract_dieu_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    dieu_final = process_dieu_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
    "            for dieu in dieu_final:\n",
    "                replaced_text.append(dieu)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "#############\n",
    "#############\n",
    "########### khoản 1, 2,3  và .... + replace\n",
    "\n",
    "def split_by_khoan_new1(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in khoan_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "#####################\n",
    "def extract_khoan_err(text):\n",
    "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "################\n",
    "def process_khoan_parts(text):\n",
    "    khoan_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    khoan_final.append(temp)\n",
    "                else:\n",
    "                    khoan_final.append('khoản ' + temp)\n",
    "\n",
    "    return khoan_final\n",
    "###############\n",
    "\n",
    "def replace_khoan_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_khoan_new1(text)\n",
    "    matches = extract_khoan_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    khoan_final = process_khoan_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for khoan in khoan_final:\n",
    "                replaced_text.append(khoan)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "#############\n",
    "#############\n",
    "################# Điểm 1,2,3 và .... + replace\n",
    "\n",
    "def split_by_diem_new1(text):\n",
    "    def extract_diem_positions(text):\n",
    "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
    "\n",
    "    diem_positions = extract_diem_positions(text)\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in diem_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "###########################\n",
    "def extract_diem_err(text):\n",
    "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "#########################\n",
    "def process_diem_parts(text):\n",
    "    diem_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    diem_final.append(temp)\n",
    "                else:\n",
    "                    diem_final.append('điểm ' + temp)\n",
    "\n",
    "    return diem_final\n",
    "#########################\n",
    "def replace_diem_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_diem_new1(text)\n",
    "    matches = extract_diem_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    diem_final = process_diem_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for diem in diem_final:\n",
    "                replaced_text.append(diem)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "def split_and_transform_dieu_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_dieu = None\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ Điều\" in split_text_part:\n",
    "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if start_dieu_match:\n",
    "                    start_dieu = start_dieu_match.group(1)\n",
    "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if end_dieu_match:\n",
    "                    end_dieu = end_dieu_match.group(1)\n",
    "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                    if start_index is not None and end_index is not None:\n",
    "                        for d in range(start_index, end_index + 1):\n",
    "                            new_split_text.append(f\"Điều {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ Điều\" in split_text_part:\n",
    "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if start_dieu_match:\n",
    "                start_dieu = start_dieu_match.group(1)\n",
    "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if end_dieu_match:\n",
    "                end_dieu = end_dieu_match.group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    for d in range(start_index, end_index + 1):\n",
    "                        new_split_text.append(f\"Điều {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_dieu_ranges(text):\n",
    "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    if not dieu_range_match:\n",
    "        return text\n",
    "\n",
    "    dieu_range_text = dieu_range_match.group(0)\n",
    "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
    "\n",
    "    # Tạo chuỗi mới từ danh sách các Điều\n",
    "    dieu_list_str = ', '.join(dieu_list)\n",
    "\n",
    "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
    "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_khoan_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_khoan = None\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ khoản\" in split_text_part.lower():\n",
    "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"khoản {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ khoản\" in split_text_part.lower():\n",
    "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"khoản {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_khoan_ranges(text):\n",
    "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for match in khoan_range_matches:\n",
    "        khoan_range_text = match.group(0)\n",
    "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
    "\n",
    "        # Tạo chuỗi mới từ danh sách các khoản\n",
    "        khoan_list_str = ', '.join(khoan_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
    "\n",
    "        # Cập nhật offset\n",
    "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_diem_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_diem = None\n",
    "\n",
    "    for i, (start, end) in enumerate(diem_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ điểm\" in split_text_part.lower():\n",
    "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"điểm {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ điểm\" in split_text_part.lower():\n",
    "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"điểm {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_diem_ranges(text):\n",
    "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for match in diem_range_matches:\n",
    "        diem_range_text = match.group(0)\n",
    "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
    "\n",
    "        # Tạo chuỗi mới từ danh sách các điểm\n",
    "        diem_list_str = ', '.join(diem_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
    "\n",
    "        # Cập nhật offset\n",
    "        offset += len(diem_list_str) - len(diem_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "########### Cắt từ điều này tới điều kia\n",
    "\n",
    "\n",
    "def split_by_dieu(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "########### Cắt từ khoản này tới khoản kia\n",
    "\n",
    "def split_by_khoan(text):\n",
    "    if text is None:\n",
    "            return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    first_khoan_start = khoan_positions[0][0]\n",
    "    if first_khoan_start > 0:\n",
    "        split_text.append(text[:first_khoan_start].strip())\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "def processing_ccpl_full(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
    "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
    "\n",
    "    return check2\n",
    "\n",
    "# Hàm trích xuất điều\n",
    "def extract_dieu(text):\n",
    "    # Find individual \"Điều\" references\n",
    "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
    "\n",
    "    # Find joined \"Điều\" lists\n",
    "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
    "\n",
    "    # Process joined \"Điều\" lists\n",
    "    for item in joined_dieu_list:\n",
    "        # Find all \"Điều\" references in the joined list\n",
    "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
    "        # Add only the unique \"Điều\" references to the main list\n",
    "        for ref in joined_dieu_refs:\n",
    "            if ref not in dieu_list:\n",
    "                dieu_list.append(ref)\n",
    "    return dieu_list\n",
    "\n",
    "# Hàm trích xuất khoản\n",
    "def extract_khoan(text):\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_khoan_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                for idx in range(int(start), int(end) + 1):\n",
    "                    final_khoan_list.append(str(idx))\n",
    "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
    "                final_khoan_list.append(part)\n",
    "    return final_khoan_list\n",
    "\n",
    "# Hàm trích xuất điểm\n",
    "def extract_diem(text):\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_diem_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
    "                final_diem_list.append(part)\n",
    "    return final_diem_list\n",
    "\n",
    "# hàm xử lý đầu ra ccpl\n",
    "def processing_output_ccpl(input_data):\n",
    "    output_array = []\n",
    "    for dieu in input_data:\n",
    "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
    "        if dieu[\"Khoan\"]:\n",
    "            for khoan in dieu[\"Khoan\"]:\n",
    "                for khoan_item in khoan[\"Khoan\"]:\n",
    "                    if khoan[\"Diem\"]:\n",
    "                        for diem in khoan[\"Diem\"]:\n",
    "                            for diem_item in diem:\n",
    "                                output_array.extend([\n",
    "                                    dieu_value,\n",
    "                                    khoan_item,\n",
    "                                    diem_item\n",
    "                                ])\n",
    "                    else:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "        else:\n",
    "            output_array.extend([\n",
    "                dieu_value,\n",
    "                \"0\",\n",
    "                \"0\"\n",
    "            ])\n",
    "    if output_array:\n",
    "        return output_array\n",
    "\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicate ccpl entries\n",
    "def remove_duplicates_ccpls(data):\n",
    "    for item in data:\n",
    "        seen = set()\n",
    "        unique_ccpls = []\n",
    "        for ccpl in item['ccpls']:\n",
    "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
    "                ccpl_tuple = (ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
    "                # ccpl['LawID'], \n",
    "                if ccpl_tuple not in seen:\n",
    "                    seen.add(ccpl_tuple)\n",
    "                    unique_ccpls.append(ccpl)\n",
    "        item['ccpls'] = unique_ccpls\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format các dạng bảng\n",
    "def table_to_markdown(table):\n",
    "    if table is None:\n",
    "        return \"\"\n",
    "    rows = []\n",
    "    headers = []\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(['th', 'td']):\n",
    "            cell_text = cell.get_text(strip=True)\n",
    "            row_data.append(cell_text)\n",
    "        if not headers:\n",
    "            headers = row_data\n",
    "        else:\n",
    "            rows.append(row_data)\n",
    "\n",
    "    return tabulate(rows, headers, tablefmt='pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm lấy q,a,ccpl\n",
    "def processing_1(h2_tags, soup_data): #response.content\n",
    "    # Tìm tất cả các thẻ <h2>\n",
    "    questions_and_answers1 = []\n",
    "    if len(h2_tags) == 1:\n",
    "        answer_text = \"\"\n",
    "        for i in range(len(h2_tags)):\n",
    "            h2_current = h2_tags[i]\n",
    "            temp_results = []\n",
    "            next_tag = h2_tags[i].find_next_sibling()\n",
    "            gay_flag = True\n",
    "            while next_tag and next_tag.name != h2_current.name:\n",
    "                if next_tag.name == 'p':\n",
    "                    p_tag = next_tag.text\n",
    "                    if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
    "                        gay_flag = False\n",
    "                    if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
    "                        for a_tag in next_tag.find_all('a'):\n",
    "                            href = a_tag.get('href')\n",
    "                            if href and (re.search(r'-d([1-9]|10)\\.html', href) or '.aspx' in href):\n",
    "                                final = processing_ccpl_full(a_tag.text)\n",
    "                                positions = split_by_dieu(final)\n",
    "                                # test = p_tag.split(positions)\n",
    "                                # print(positions)\n",
    "                                lawtitle = positions[-1]\n",
    "                                test = p_tag.split(lawtitle)\n",
    "                                # print(lawtitle)\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                if \"điều\" in test[0].lower():\n",
    "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                    final = processing_ccpl_full(test[0])\n",
    "                                    positions = split_by_dieu(final)\n",
    "                                    data_ccpl = []\n",
    "                                    for dieu in positions:\n",
    "                                        if \"điều\" in dieu.lower():\n",
    "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                            # Tìm anchor trong href\n",
    "                                            khoan_list = split_by_khoan(dieu)\n",
    "                                            for khoan in khoan_list:\n",
    "                                                if \"khoản\" in khoan.lower():\n",
    "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                    if \"điểm\" in khoan.lower():\n",
    "                                                        diem_dict = extract_diem(khoan)\n",
    "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                            data_ccpl.append(dieu_dict)\n",
    "                                            # print(data_ccpl)\n",
    "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                            if len(ccpl_temp) > 3:\n",
    "                                                for i in range(0, len(ccpl_temp), 3):\n",
    "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                    chunk = ccpl_temp[i:i+3]\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    # print(ccpl_temp)\n",
    "                                                    if len(chunk) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            # \"url\": href,\n",
    "                                                            # \"anchor\": anchor,\n",
    "                                                            # \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": chunk[0],\n",
    "                                                            \"Khoan\": chunk[1],\n",
    "                                                            \"Diem\": chunk[2]\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "                                            else:\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                if len(ccpl_temp) > 0:\n",
    "                                                    dictionary = {\n",
    "                                                        # \"url\": href,\n",
    "                                                        # \"anchor\": anchor,\n",
    "                                                        # \"LawID\": law_id,\n",
    "                                                        \"LawTitle\": lawtitle,\n",
    "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                        \"Khoan\": ccpl_temp[1],\n",
    "                                                        \"Diem\": ccpl_temp[2]\n",
    "                                                    }\n",
    "                                                    temp_results.append(dictionary)\n",
    "\n",
    "                                else:\n",
    "                                    temp_else = {}\n",
    "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                    # law_title = a_tag.get_text()\n",
    "                                    # temp_else[\"url\"] = href\n",
    "                                    # temp_else[\"anchor\"] = anchor\n",
    "                                    # temp_else[\"LawID\"] = law_id\n",
    "                                    temp_else[\"LawTitle\"] = lawtitle\n",
    "                                    temp_else[\"Dieu\"] = \"0\"\n",
    "                                    temp_else[\"Khoan\"] = \"0\"\n",
    "                                    temp_else[\"Diem\"] = \"0\"\n",
    "                                    temp_results.append(temp_else)\n",
    "                        gay_flag = True\n",
    "                next_tag = next_tag.find_next_sibling()\n",
    "            ccpls = []\n",
    "            ccpls.extend(temp_results)\n",
    "        answer = soup_data.find('div', id=\"tra-loi\")\n",
    "\n",
    "        if not answer:\n",
    "            answer = soup_data.find('section', class_ = \"news-content\")\n",
    "            \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', id=\"content_article\")\n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', id=\"article-content\")\n",
    "            \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', id=\"news-bodyhtml\")\n",
    "        \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"article-detail\")\n",
    "        \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"content-detail\")  \n",
    "            \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"entry-content single-page\")  \n",
    "            \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"noidung-chitiet\")  \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"the-article-body entry\")  \n",
    "        if not answer:\n",
    "            answer = soup_data.find('div', class_=\"entry-hoi-dap\")  \n",
    "\n",
    "        # # wikihow\n",
    "        # if not answer:\n",
    "        #     answer = soup_data.find('div', id=\"mf-section-1\")       \n",
    "            \n",
    "        #trang báo\n",
    "        if not answer:\n",
    "            answer = soup_data.find('article', class_=\"fck_detail\")\n",
    "  \n",
    "        if answer:    \n",
    "            # print(answer)\n",
    "            for temp in answer:\n",
    "                if not isinstance(temp, Tag) and temp.name == 'table':\n",
    "                    answer_text += table_to_markdown(temp) + \"\\n\"\n",
    "                    # print(answer_text)\n",
    "                else:\n",
    "                    answer_text += temp.text + \"\\n\"\n",
    "                \n",
    "            questions_and_answers1.append({\n",
    "                \"question\": unicodedata.normalize(\"NFC\", h2_current.text),\n",
    "                \"answer\": re.sub(r'\\n{2,}', '\\n', answer_text.strip()),\n",
    "                \"ccpls\": []\n",
    "            })\n",
    "    else:\n",
    "        for i in range(len(h2_tags)):\n",
    "            h2_current = h2_tags[i]\n",
    "            # Lấy văn bản cho câu trả lời\n",
    "            answer_text = \"\"\n",
    "            next_element = h2_current.next_sibling\n",
    "\n",
    "            while next_element and (not isinstance(next_element, Tag) or next_element.name != h2_current.name):\n",
    "                if next_element and next_element.name == 'table':\n",
    "                    answer_text += table_to_markdown(next_element) + \"\\n\"\n",
    "                    # answer_text += answer.strip() + \"\\n\"\n",
    "                    # Kiểm tra nếu next_element là NavigableString\n",
    "                    # print(answer_text)\n",
    "                elif isinstance(next_element, NavigableString):\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element)\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                elif next_element and next_element.name:\n",
    "                    answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
    "                    answer_text += answer.strip() + \"\\n\"\n",
    "                next_element = next_element.next_sibling\n",
    "\n",
    "            temp_results = []\n",
    "            next_tag = h2_tags[i].find_next_sibling()\n",
    "            gay_flag = True\n",
    "            while next_tag and next_tag.name != h2_current.name:\n",
    "                if next_tag.name == 'p':\n",
    "                    p_tag = next_tag.text\n",
    "                    if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
    "                        gay_flag = False\n",
    "                    if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
    "                        for a_tag in next_tag.find_all('a'):\n",
    "                            href = a_tag.get('href')\n",
    "                            if href and (re.search(r'-d([1-9]|10)\\.html', href) or '.aspx' in href):\n",
    "                                final = processing_ccpl_full(a_tag.text)\n",
    "                                positions = split_by_dieu(final)\n",
    "                                lawtitle = positions[-1]\n",
    "                                test = p_tag.split(lawtitle)\n",
    "                                # print(lawtitle)\n",
    "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                if \"điều\" in test[0].lower():\n",
    "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                    final = processing_ccpl_full(test[0])\n",
    "                                    positions = split_by_dieu(final)\n",
    "                                    data_ccpl = []\n",
    "                                    for dieu in positions:\n",
    "                                        if \"điều\" in dieu.lower():\n",
    "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                            # Tìm anchor trong href\n",
    "                                            khoan_list = split_by_khoan(dieu)\n",
    "                                            for khoan in khoan_list:\n",
    "                                                if \"khoản\" in khoan.lower():\n",
    "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                    if \"điểm\" in khoan.lower():\n",
    "                                                        diem_dict = extract_diem(khoan)\n",
    "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                            data_ccpl.append(dieu_dict)\n",
    "                                            # print(data_ccpl)\n",
    "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                            if len(ccpl_temp) > 3:\n",
    "                                                for i in range(0, len(ccpl_temp), 3):\n",
    "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                    chunk = ccpl_temp[i:i+3]\n",
    "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                    # print(ccpl_temp)\n",
    "                                                    if len(chunk) > 0:\n",
    "                                                        dictionary = {\n",
    "                                                            # \"url\": href,\n",
    "                                                            # \"anchor\": anchor,\n",
    "                                                            # \"LawID\": law_id,\n",
    "                                                            \"LawTitle\": lawtitle,\n",
    "                                                            \"Dieu\": chunk[0],\n",
    "                                                            \"Khoan\": chunk[1],\n",
    "                                                            \"Diem\": chunk[2]\n",
    "                                                        }\n",
    "                                                        temp_results.append(dictionary)\n",
    "                                            else:\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                if len(ccpl_temp) > 0:\n",
    "                                                    dictionary = {\n",
    "                                                        # \"url\": href,\n",
    "                                                        # \"anchor\": anchor,\n",
    "                                                        # \"LawID\": law_id,\n",
    "                                                        \"LawTitle\": lawtitle,\n",
    "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                        \"Khoan\": ccpl_temp[1],\n",
    "                                                        \"Diem\": ccpl_temp[2]\n",
    "                                                    }\n",
    "                                                    temp_results.append(dictionary)\n",
    "\n",
    "                                else:\n",
    "                                    temp_else = {}\n",
    "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                    # law_title = a_tag.get_text()\n",
    "                                    # temp_else[\"url\"] = href\n",
    "                                    # temp_else[\"anchor\"] = anchor\n",
    "                                    # temp_else[\"LawID\"] = law_id\n",
    "                                    temp_else[\"LawTitle\"] = lawtitle\n",
    "                                    temp_else[\"Dieu\"] = \"0\"\n",
    "                                    temp_else[\"Khoan\"] = \"0\"\n",
    "                                    temp_else[\"Diem\"] = \"0\"\n",
    "                                    temp_results.append(temp_else)\n",
    "                        gay_flag = True\n",
    "                next_tag = next_tag.find_next_sibling()\n",
    "            ccpls = []\n",
    "            ccpls.extend(temp_results)\n",
    "            questions_and_answers1.append({\n",
    "                \"question\": unicodedata.normalize(\"NFC\", h2_current.text),\n",
    "                \"answer\": re.sub(r'\\n{2,}', '\\n', answer_text.strip()),\n",
    "                \"ccpls\": ccpls\n",
    "            })\n",
    "    # Danh sách các mẫu regex cần tìm và xóa từ đó đến hết câu trả lời\n",
    "    patterns = [\n",
    "        re.compile(r\"\\s*\\n{2}\\s*Về\\s*vấn\\s*đề\\s*này,\"),  # Khớp với ít nhất 2 ký tự xuống dòng trước và giữa các từ\n",
    "        re.compile(r\"Về vấn đề này,\\s*THƯ VIỆN PHÁP LUẬT\\s*giải đáp như sau:\"),\n",
    "        re.compile(r\"\\*\\*\\*\\*\", re.MULTILINE),\n",
    "        re.compile(r\"THƯ VIỆN PHÁP LUẬT\"),\n",
    "        re.compile(r\"^Xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"^>> Xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Xem chi tiết tại.*\", re.MULTILINE),\n",
    "        re.compile(r\"^>> Tham khảo.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Tham khảo.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Chọn lĩnh vực để xem văn bản liên quan.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Trân trọng.*\", re.MULTILINE),\n",
    "        re.compile(r\"^HỎI ĐÁP PHÁP LUẬT LIÊN QUAN.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Trân trọng!*\", re.MULTILINE),\n",
    "        re.compile(r\"^Các bài viết liên quan.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Để được tư vấn chi tiết xin vui lòng liên hệ.*\", re.MULTILINE),\n",
    "        re.compile(r\"^TIN LIÊN QUAN.*\", re.MULTILINE),\n",
    "        re.compile(r\"^>>.*\", re.MULTILINE)\n",
    "    ]\n",
    "    for item in questions_and_answers1:\n",
    "        answer_text = item[\"answer\"]\n",
    "        # Khởi tạo start_index với giá trị -1 (không tìm thấy)\n",
    "        start_index = -1\n",
    "        # Kiểm tra từng mẫu regex và cập nhật start_index nếu tìm thấy\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(answer_text)\n",
    "            if match:\n",
    "                index = match.start()\n",
    "                # Nếu start_index chưa được cập nhật hoặc tìm thấy index nhỏ hơn (gần đầu chuỗi hơn)\n",
    "                if start_index == -1 or index < start_index:\n",
    "                    start_index = index\n",
    "\n",
    "        # Nếu tìm thấy bất kỳ mẫu nào, xóa từ đoạn đó đến hết câu trả lời\n",
    "        if start_index != -1:\n",
    "            cleaned_answer = answer_text[:start_index].strip()\n",
    "            item[\"answer\"] = cleaned_answer\n",
    "    # print(json.dumps(answer_text, ensure_ascii=False, indent=5))\n",
    "    return json.dumps(questions_and_answers1, ensure_ascii=False, indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm tìm tất cả câu hỏi\n",
    "def find_question(soup_data):\n",
    "    questions = []\n",
    "    for h2_tags in find_h2_tags(soup_data):\n",
    "        questions.append(h2_tags.text)\n",
    "        # print(h2_tags)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm xử lý output\n",
    "def extract_data(soup_data):\n",
    "    file1_data = json.loads(processing_1(find_h2_tags(soup_data),soup_data))\n",
    "    # Chỉ giữ lại các mục có câu trả lời\n",
    "    filtered_questions_and_answers = [qa for qa in file1_data if 'answer' in qa and qa['answer']]\n",
    "    filtered_questions_and_answers = [qa for qa in filtered_questions_and_answers if 'question' in qa and qa['question']]\n",
    "    # Remove duplicates\n",
    "    questions_and_answers_final = remove_duplicates_ccpls(filtered_questions_and_answers)\n",
    "    return questions_and_answers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-None-Y8DoDJRGH63XFz1edTyaT3BlbkFJMtiwHXQBmx5addWF1b57\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_gpt = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'luatvietnam.vn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lặp qua từng dòng của DataFrame và lấy giá trị của từng dòng\n",
    "# for index, row in df.iterrows():\n",
    "#     input = index\n",
    "input = ' Người dưới 18 tuổi có được tham gia BHXH bắt buộc không?'\n",
    "\n",
    "postagging = postagging_keyword(input)\n",
    "google_search_text = '; '.join(postagging)\n",
    "# print(google_search_text)\n",
    "# print(google_search_text)\n",
    "gooole_request = google_search(google_search_text)\n",
    "# print(gooole_request)\n",
    "# gooole_request = google_search(postagging)\n",
    "matching_result = find_max_result_google_search(input, gooole_request, model)\n",
    "# print(matching_result)\n",
    "# matching_result = find_most_similar_sentence(postagging ,gooole_request)\n",
    "output = ''\n",
    "# print(matching_result)\n",
    "if matching_result:\n",
    "    data_soup = crawl_data(matching_result)\n",
    "    question_all = find_question(data_soup)\n",
    "    # question = find_most_similar_question(postagging, question_all)\n",
    "    question = find_max_result_question(input, question_all, model)\n",
    "    # print(question)\n",
    "    output = extract_data(data_soup)\n",
    "result = \"\"\n",
    "if output:\n",
    "    for temp in output:\n",
    "        if temp['question'] == question:\n",
    "            # result = temp['answer'].strip()\n",
    "            result = (temp['answer'].strip())\n",
    "            # print(matching_result.get('href'))\n",
    "            # if temp['ccpls']:\n",
    "            #     print(temp['ccpls'])\n",
    "            # break\n",
    "else:\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'luatvietnam.vn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseOutputParser.invoke() got an unexpected keyword argument 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 49\u001b[0m\n\u001b[0;32m     42\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     43\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mbạn là một luật gia sư tại việt nam, hãy trả lời trung thực, không bịa ra câu trả lời.\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m                  Nếu bên dưới có thông tin thì hãy dựa vào thông tin bên dưới để trả lời câu hỏi cho người dùng, sau đó hãy đưa ra kết luận cho người dùng: \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m+\u001b[39m result),\n\u001b[0;32m     45\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m),\n\u001b[0;32m     46\u001b[0m ]\n\u001b[0;32m     48\u001b[0m result_gpt \u001b[38;5;241m=\u001b[39m model_gpt\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_gpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ccpl:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m ccpl:\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseOutputParser.invoke() got an unexpected keyword argument 'stream'"
     ]
    }
   ],
   "source": [
    "# Lặp qua từng dòng của DataFrame và lấy giá trị của từng dòng\n",
    "# for index, row in df.iterrows():\n",
    "#     input = index\n",
    "input = 'Tách sổ đỏ cho con có mất phí không?'\n",
    "\n",
    "postagging = postagging_keyword(input)\n",
    "google_search_text = '; '.join(postagging)\n",
    "# print(google_search_text)\n",
    "# print(google_search_text)\n",
    "gooole_request = google_search(google_search_text)\n",
    "# print(gooole_request)\n",
    "# gooole_request = google_search(postagging)\n",
    "matching_result = find_max_result_google_search(input, gooole_request, model)\n",
    "# print(matching_result)\n",
    "# matching_result = find_most_similar_sentence(postagging ,gooole_request)\n",
    "output = ''\n",
    "# print(matching_result)\n",
    "if matching_result:\n",
    "    data_soup = crawl_data(matching_result)\n",
    "    question_all = find_question(data_soup)\n",
    "    # question = find_most_similar_question(postagging, question_all)\n",
    "    question = find_max_result_question(input, question_all, model)\n",
    "    # print(question)\n",
    "    output = extract_data(data_soup)\n",
    "result = \"\"\n",
    "ccpl = []\n",
    "if output:\n",
    "    for temp in output:\n",
    "        if temp['question'] == question:\n",
    "            # result = temp['answer'].strip()\n",
    "            result = (temp['answer'].strip())\n",
    "            if temp['ccpls']:\n",
    "                ccpl.append(temp['ccpls'])\n",
    "            # print(matching_result.get('href'))\n",
    "            # if temp['ccpls']:\n",
    "            #     print(temp['ccpls'])\n",
    "            # break\n",
    "else:\n",
    "    print('')\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"bạn là một luật gia sư tại việt nam, hãy trả lời trung thực, không bịa ra câu trả lời.\n",
    "                  Nếu bên dưới có thông tin thì hãy dựa vào thông tin bên dưới để trả lời câu hỏi cho người dùng, sau đó hãy đưa ra kết luận cho người dùng: \"\"\" + result),\n",
    "    HumanMessage(content= input),\n",
    "]\n",
    "\n",
    "result_gpt = model_gpt.invoke(messages)\n",
    "print(parser.invoke(result_gpt)\n",
    "if ccpl:\n",
    "    for temp in ccpl:\n",
    "        print(temp)\n",
    "print(matching_result.get('href'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minh-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
